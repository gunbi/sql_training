# 执行环境配置
execution:
  planner: blink  # 使用 blink planner
  type: streaming # 流处理模式
  parallelism: 1

# 表定义
tables:
  # Kafka 源表
  - name: kafka_novels
    type: source
    update-mode: append
    schema:
      - name: id
        type: BIGINT
      - name: createTime
        type: BIGINT  # 改为BIGINT以匹配物理类型
      - name: category
        type: STRING
      - name: novelName
        type: STRING
      - name: authorName
        type: STRING
      - name: authorLevel
        type: STRING
      - name: updateTime
        type: BIGINT  # 改为BIGINT以匹配物理类型
      - name: wordCount
        type: BIGINT
      - name: monthlyTicket
        type: BIGINT
      - name: totalClick
        type: BIGINT
      - name: status
        type: STRING
      - name: completeTime
        type: BIGINT  # 改为BIGINT以匹配物理类型
      # 定义 completeTime 为 Event Time
      - name: event_time
        type: TIMESTAMP
        rowtime:
          timestamps:
            type: from-field
            from: completeTime
          watermarks:
            type: periodic-bounded
            delay: 5000
      # 定义处理时间
      - name: proc_time
        type: TIMESTAMP
        proctime: true
    connector:
      property-version: 1
      type: kafka
      version: universal    # 使用通用版本，支持 Kafka 3.x
      topic: test-topic
      startup-mode: earliest-offset
      properties:
        - key: bootstrap.servers
          value: localhost:29092
        - key: auto.offset.reset
          value: earliest
    format:
      property-version: 1
      type: json
      schema: "ROW(id BIGINT, createTime BIGINT, category STRING, novelName STRING, authorName STRING, authorLevel STRING, updateTime BIGINT, wordCount BIGINT, monthlyTicket BIGINT, totalClick BIGINT, status STRING, completeTime BIGINT)"

  # MySQL 结果表
  - name: mysql_novels
    type: sink
    schema:
      - name: id
        type: BIGINT
      - name: create_time
        type: TIMESTAMP
      - name: category
        type: STRING
      - name: novel_name
        type: STRING
      - name: author_name
        type: STRING
      - name: author_level
        type: STRING
      - name: update_time
        type: TIMESTAMP
      - name: word_count
        type: BIGINT
      - name: monthly_ticket
        type: BIGINT
      - name: total_click
        type: BIGINT
      - name: status
        type: STRING
      - name: complete_time
        type: TIMESTAMP
    connector:
      property-version: 1
      type: jdbc
      driver: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/novels?useSSL=false&allowPublicKeyRetrieval=true&serverTimezone=UTC
      table: novels
      username: flink
      password: flink123
      write.flush.max-rows: 1
      write.flush.interval: 1000


# 示例查询
# 基于事件时间的窗口查询
# SELECT 
#   TUMBLE_START(event_time, INTERVAL '7' DAY ) as window_start,
#   category,
#   COUNT(*) as novel_count,
#   AVG(CAST(wordCount AS DOUBLE)) as avg_word_count
# FROM kafka_novels
# GROUP BY 
#   TUMBLE(event_time, INTERVAL '7' DAY),
#   category;
#
# 基于处理时间的窗口查询
# SELECT 
#   HOP_START(proc_time, INTERVAL '1' MINUTE, INTERVAL '1' HOUR) as window_start,
#   HOP_END(proc_time, INTERVAL '1' MINUTE, INTERVAL '1' HOUR) as window_end,
#   category,
#   COUNT(*) as novel_count
# FROM kafka_novels
# GROUP BY 
#   HOP(proc_time, INTERVAL '1' MINUTE, INTERVAL '1' HOUR),
#   category;
#
# 使用INSERT INTO来处理数据
# INSERT INFO mysql_novels
# SELECT
#   id,
#   TO_TIMESTAMP(FROM_UNIXTIME(createTime/1000)) as create_time,
#   category,
#   novelName as novel_name,
#   authorName as author_name,
#   authorLevel as author_level,
#   TO_TIMESTAMP(FROM_UNIXTIME(updateTime/1000)) as update_time,
#   wordCount as word_count,
#   monthlyTicket as monthly_ticket,
#   totalClick as total_click,
#   status,
#   TO_TIMESTAMP(FROM_UNIXTIME(completeTime/1000)) as complete_time
# FROM kafka_novels;